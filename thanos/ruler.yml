### Here you can set and adjust the alerts and rules for all of CPaaS's components, kubernets the thanos ones. 

groups:

# MongoDB
- name: MongoDB 
  rules:
  - alert: '[MongoDB / Edge Instance] Instance Down'
    annotations:
      description: Mongo server detected down by instance
    expr: |
      mongodb_up != 1
    for: 10m
    labels:
      severity: critical
  - alert: '[MongoDB / Edge Instance] Uptime less than one hour'
    annotations:
      description: Mongo server detected down by instance
    expr: |
      mongodb_uptime < 3600
    for: 10m
    labels:
      severity: critical
  - alert: '[MongoDB / Edge Instance] High Latency'
    annotations:
      description: High latency in instance
    expr: |
      rate(mongodb_mongod_op_latencies_latency_total[5m]) / rate(mongodb_mongod_op_latencies_ops_total[5m]) > 250000
    for: 10m
    labels:
      severity: critical
  - alert: '[MongoDB / Edge Instance] Recurrent Cursor Timeout'
    annotations:
      description: Recurrent cursors timeout in instance
    expr: |
      irate(mongodb_metrics_cursor_timedOut[5m])  > 0
    for: 30m
    labels:
      severity: critical
  - alert: '[MongoDB / Edge Instance] Recurrent Memory Page Faults'
    annotations:
      description: Recurrent cursors timeout in instance
    expr: |
      sum(irate(mongodb_extra_info_page_faults[5m])) > 0
    for: 30m
    labels:
      severity: critical

# kafka

- name: Kafka
  rules:
  - alert: '[Kafka] No Leader'
    annotations:
      description: There is no ActiveController or 'leader' in the Kafka cluster.
    expr: |
      sum(kafka_controller_kafkacontroller_activecontrollercount) < 1
    for: 5m
    labels:
      severity: critical
  - alert: '[Kafka] Too Many Leaders'
    annotations:
      description: There is more than one ActiveController or 'leader' in the Kafka
        cluster.
    expr: |
      kafka_controller_kafkacontroller_activecontrollercount > 1
    for: 10m
    labels:
      severity: critical
  - alert: '[Kafka] Offline Partitions'
    annotations:
      description: There are one or more Offline Partitions. These partitions donâ€™t
        have an active leader and are hence not writable or readable.
    expr: |
      kafka_controller_kafkacontroller_offlinepartitionscount > 0
    for: 5m
    labels:
      severity: critical
  - alert: '[Kafka] Under Replicated Partitions'
    annotations:
      description: There are one or more Under Replicated Partitions.
    expr: |
      kafka_server_replicamanager_underreplicatedpartitions > 0
    for: 10m
    labels:
      severity: warning
  - alert: '[Kafka] Under In-Sync Replicated Partitions'
    annotations:
      description: There are one or more Under In-Sync Replicated Partitions. These
        partitions will be unavailable to producers who use 'acks=all'.
    expr: |
      kafka_cluster_partition_underreplicated > 0
    for: 10m
    labels:
      severity: warning
  - alert: '[Kafka] ConsumerGroup Lag Not Decreasing'
    annotations:
      description: The ConsumerGroup lag is not decreasing. The Consumers might
        be down, failing to process the messages and continuously retrying, or their
        consumption rate is lower than the production rate of messages.
    expr: |
      kafka_server_fetcherlagmetrics_consumerlag > 0 and delta(kafka_server_fetcherlagmetrics_consumerlag[2m]) >= 0
    for: 15m
    labels:
      severity: warning
  - alert: '[Kafka] Consumergroup lag to large'
    annotations:
      description: Consumer group {{ $labels.consumergroup}} lag is too big ({{
        $value }}) on topic {{ $labels.topic }}/partition {{ $labels.partition }}
      summary: Consumer group lag is too big
    expr: kafka_server_fetcherlagmetrics_consumerlag > 1000
    for: 10s
    labels:
      severity: warning
  - alert: '[Kafka] Producer High ThrottleTime By Client-Id'
    annotations:
      description: The Producer has reached its quota and has high throttle time.
        Applicable when Client-Id-only quotas are being used.
    expr: |
      max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Producer High ThrottleTime By User'
    annotations:
      description: The Producer has reached its quota and has high throttle time.
        Applicable when User-only quotas are being used.
    expr: |
      max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Producer High ThrottleTime By User And Client-Id'
    annotations:
      description: The Producer has reached its quota and has high throttle time.
        Applicable when Client-Id + User quotas are being used.
    expr: |
      max (kafka_server_producer__client_throttle_time) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Consumer High ThrottleTime By Client-Id'
    annotations:
      description: The Consumer has reached its quota and has high throttle time.
        Applicable when Client-Id-only quotas are being used.
    expr: |
      max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Consumer High ThrottleTime By User'
    annotations:
      description: The Consumer has reached its quota and has high throttle time.
        Applicable when User-only quotas are being used.
    expr: |
      max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Consumer High ThrottleTime By User And Client-Id'
    annotations:
      description: The Consumer has reached its quota and has high throttle time.
        Applicable when Client-Id + User quotas are being used.
    expr: |
      max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] Mirror Maker Container Down'
    annotations:
      description: All Kafka Mirror Maker containers have been down or in CrashLookBackOff
        status for 3 minutes
      summary: All Kafka Mirror Maker containers down or in CrashLookBackOff status
    expr: absent(container_last_seen{container=~".+-mirrormaker2",pod=~".+-mirrormaker2.+"})
    for: 3m
    labels:
      severity: major
  - alert: '[Kafka] Under Min Isr Partition Count'
    annotations:
      description: There are {{ $value }} partitions under the min ISR on {{ $labels.kubernetes_pod_name
        }}
      summary: Kafka under min ISR partitions
    expr: kafka_server_replicamanager_underminisrpartitioncount > 0
    for: 10s
    labels:
      severity: warning
  - alert: '[Kafka] Cluster operator container down'
    annotations:
      description: The Cluster Operator has been down for longer than 90 seconds
      summary: Cluster Operator down
    expr: count((container_last_seen{container="strimzi-cluster-operator"} > (time()
      - 90))) < 1 or absent(container_last_seen{container="strimzi-cluster-operator"})
    for: 1m
    labels:
      severity: major
  - alert: '[Kafka]  KafkaContainerRestartedInTheLast5Minutes'
    annotations:
      description: One or more Kafka containers were restarted too often within
        the last 5 minutes
      summary: One or more Kafka containers restarted too often
    expr: count(count_over_time(container_last_seen{container="kafka"}[5m])) > 2
      * count(container_last_seen{container="kafka",pod=~".+-kafka-[0-9]+"})
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] AvgRequestLatency'
    annotations:
      description: The average request latency is {{ $value }} on {{ $labels.kubernetes_pod_name
        }}
      summary: Zookeeper average request latency
    expr: zookeeper_avgrequestlatency > 10
    for: 10s
    labels:
      severity: warning
  - alert: '[Kafka] OutstandingRequests'
    annotations:
      description: There are {{ $value }} outstanding requests on {{ $labels.kubernetes_pod_name
        }}
      summary: Zookeeper outstanding requests
    expr: zookeeper_outstandingrequests > 10
    for: 10s
    labels:
      severity: warning
  - alert: '[Kafka Eventhub EDGE] Zookeeper Running Out Of Space less than 15% free'
    annotations:
      description: There are only {{ $value }} bytes available at {{ $labels.persistentvolumeclaim
        }} PVC
      summary: Zookeeper is running out of free disk space
    expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"}
      / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"})
      < 0.15
    for: 10s
    labels:
      severity: warning
  - alert: '[Kafka] ZookeeperContainerRestartedInTheLast5Minutes'
    annotations:
      description: One or more Zookeeper containers were restarted too often within
        the last 5 minutes. This alert can be ignored when the Zookeeper cluster
        is scaling up
      summary: One or more Zookeeper containers were restarted too often
    expr: count(count_over_time(container_last_seen{container="zookeeper"}[5m]))
      > 2 * count(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
    for: 5m
    labels:
      severity: warning
  - alert: '[Kafka] ZookeeperContainersDown'
    annotations:
      description: All zookeeper containers in the Zookeeper pods have been down
        or in CrashLookBackOff status for 3 minutes
      summary: All zookeeper containers in the Zookeeper pods down or in CrashLookBackOff
        status
    expr: absent(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
    for: 3m
    labels:
      severity: major
  - alert: '[Kafka]  TopicOperatorContainerDown'
    annotations:
      description: Container topic-operator in Entity Operator pod has been or in
        CrashLookBackOff status for 3 minutes
      summary: Container topic-operator in Entity Operator pod down or in CrashLookBackOff
        status
    expr: absent(container_last_seen{container="topic-operator",pod=~".+-entity-operator-.+"})
    for: 3m
    labels:
      severity: major
  - alert: '[Kafka] UserOperatorContainerDown'
    annotations:
      description: Container user-operator in Entity Operator pod have been down
        or in CrashLookBackOff status for 3 minutes
      summary: Container user-operator in Entity Operator pod down or in CrashLookBackOff
        status
    expr: absent(container_last_seen{container="user-operator",pod=~".+-entity-operator-.+"})
    for: 3m
    labels:
      severity: major
  - alert: '[Kafka]  EntityOperatorTlsSidecarContainerDown'
    annotations:
      description: Container tls-sidecar in Entity Operator pod have been down or
        in CrashLookBackOff status for 3 minutes
      summary: Container tls-sidecar Entity Operator pod down or in CrashLookBackOff
        status
    expr: absent(container_last_seen{container="tls-sidecar",pod=~".+-entity-operator-.+"})
    for: 3m
    labels:
      severity: major
  - alert: '[Kafka] ConnectContainersDown'
    annotations:
      description: All Kafka Connect containers have been down or in CrashLookBackOff
        status for 3 minutes
      summary: All Kafka Connect containers down or in CrashLookBackOff status
    expr: absent(container_last_seen{container=~".+-connect",pod=~".+-connect-.+"})
    for: 3m
    labels:
      severity: major


### Kubernetes-resources

- name: kubernetes-resources
  rules:
  - alert: KubeCPUOvercommit
    annotations:
      description: Cluster has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
      summary: Cluster has overcommitted CPU resource requests.
    expr: |-
      sum by (namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics", phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"})) > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeMemoryOvercommit
    annotations:
      description: Cluster has overcommitted memory resource requests for Pods
        by {{ $value | humanize }} bytes and cannot tolerate node failure.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
      summary: Cluster has overcommitted memory resource requests.
    expr: |-
      sum(namespace_memory:kube_pod_container_resource_requests:sum{}) - (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
      and
      (sum(kube_node_status_allocatable{resource="memory"}) - max(kube_node_status_allocatable{resource="memory"})) > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeCPUQuotaOvercommit
    annotations:
      description: Cluster has overcommitted CPU resource requests for Namespaces.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
      summary: Cluster has overcommitted CPU resource requests.
    expr: |-
      sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"}))
        /
      sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"})
        > 1.5
    for: 5m
    labels:
      severity: warning
  - alert: KubeMemoryQuotaOvercommit
    annotations:
      description: Cluster has overcommitted memory resource requests for Namespaces.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
      summary: Cluster has overcommitted memory resource requests.
    expr: |-
      sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"}))
        /
      sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"})
        > 1.5
    for: 5m
    labels:
      severity: warning
  - alert: KubeQuotaAlmostFull
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
      summary: Namespace quota is going to be full.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 0.9 < 1
    for: 15m
    labels:
      severity: info
  - alert: KubeQuotaFullyUsed
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
      summary: Namespace quota is fully used.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        == 1
    for: 15m
    labels:
      severity: info
  - alert: KubeQuotaExceeded
    annotations:
      description: Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
      summary: Namespace quota has exceeded the limits.
    expr: |-
      kube_resourcequota{job="kube-state-metrics", type="used"}
        / ignoring(instance, job, type)
      (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
        > 1
    for: 15m
    labels:
      severity: warning
  - alert: CPUThrottlingHigh
    annotations:
      description: '{{ $value | humanizePercentage }} throttling of CPU in namespace
        {{ $labels.namespace }} for container {{ $labels.container }} in pod {{
        $labels.pod }}.'
      responsible: DEVOPS
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
      summary: Processes experience elevated CPU throttling.
    expr: "sum by (container, pod, namespace) (increase(container_cpu_cfs_throttled_periods_total{container!=\"\",namespace!=\"dataprotection-microsoft\",
      container!=\"extension-agent\", pod!~\"cp-portal-kong.*\"}[5m])) /\tsum
      by (container, pod, namespace) (increase(container_cpu_cfs_periods_total[5m]))
      > (80 / 100)"
    for: 15m
    labels:
      severity: info


### Consul

- name: Consul cluster
  rules:
  - alert: ConsulServiceHealthcheckFailed
    annotations:
      description: |-
        Service:  {{ $labels.service_name  }} Healthcheck:  {{ $labels.service_id  }}
          VALUE =  {{ $value  }}
          LABELS =  {{ $labels  }}
      summary: Consul service healthcheck failed (instance  {{ $labels.instance  }})
    expr: consul_catalog_service_node_healthy == 0
    for: 1m
    labels:
      severity: critical

  - alert: ConsulMissingMasterNode
    annotations:
      description: |-
        Numbers of consul raft peers should be 3, in order to preserve quorum.
          VALUE =  {{ $value  }}
          LABELS =  {{ $labels  }}
      summary: Consul missing master node (instance  {{ $labels.instance  }})
    expr: consul_raft_peers < 3
    for: 0m
    labels:
      severity: critical
  - alert: ConsulAgentUnhealthy
    annotations:
      description: |-
        A Consul agent is down
          VALUE =  {{ $value  }}
          LABELS =  {{ $labels  }}
      summary: Consul agent unhealthy (instance  {{ $labels.instance  }})
    expr: consul_health_node_status{status="critical"} == 1
    for: 0m
    labels:
      severity: critical
  - alert: Consul Exporter down
    annotations:
      description: |-
        Consul exporter is down
          VALUE =  {{ $value  }}
          LABELS =  {{ $labels  }}
      summary: Consul exporter down (instance  {{ $labels.instance  }})
    expr: consul_up == 0
    for: 1m
    labels:
      severity: warning

- name: Consul
  rules:
  - alert: '[Consul] High 4xx RequestError Rate'
    annotations:
      description: High 4xx RequestError Rate
    expr: |
      sum (rate(envoy_cluster_upstream_rq_xx{response_code_class=~"4.."}[5m]))/ sum (irate(envoy_cluster_upstream_rq_xx[5m]))> 0.05
    for: 5m
    labels:
      severity: critical
  - alert: '[Consul] High Request Latency'
    annotations:
      description: Envoy High Request Latency
    expr: |
      histogram_quantile(0.95,sum(rate(envoy_cluster_upstream_cx_connect_ms_bucket[5m])) by (le)) > 0.25
    for: 5m
    labels:
      severity: critical
  - alert: '[Consul] High Response Latency'
    annotations:
      description: Envoy High Response Latency
    expr: |
      histogram_quantile(0.95,sum(rate(envoy_cluster_upstream_rq_time_bucket[5m])) by (le)) > 0.25
    for: 5m
    labels:
      severity: critical
  - alert: '[Envoy] HighRequestLatency'
    annotations:
      description: Latency exceeding 100ms for the past 5 minutes.
      summary: High request latency on Envoy
    expr: envoy_http_downstream_rq_time_bucket{job="envoy-sidecar", le="0.5"} >
      0.1
    for: 5m
    labels:
      severity: warning
  - alert: '[Envoy] HighErrorRate'
    annotations:
      description: Error rate exceeding 5% for the past 10 minutes.
      summary: High error rate on Envoy
    expr: irate(envoy_http_downstream_rq_xx{job="envoy-sidecar"}[5m]) > 0.05
    for: 10m
    labels:
      severity: critical
  - alert: '[Envoy] HighConnections'
    annotations:
      description: More than 500 active connections for the past 15 minutes.
      summary: High number of active connections on Envoy
    expr: envoy_cluster_upstream_cx_active{job="envoy-sidecar"} > 500
    for: 15m
    labels:
      severity: warning
  - alert: '[Envoy] HighRequestRate'
    annotations:
      description: More than 100 requests per second for the past 5 minutes.
      summary: High request rate on Envoy
    expr: rate(envoy_http_downstream_rq_total{job="envoy"}[1m]) > 100
    for: 5m
    labels:
      severity: warning
  - alert: '[Envoy] DroppedRequests'
    annotations:
      description: More than 5 upstream requests dropped for the past 15 minutes.
      summary: High number of dropped requests on Envoy
    expr: envoy_cluster_upstream_rq_tx_reset{job="envoy-sidecar"} > 5
    for: 15m
    labels:
      severity: warning
  - alert: '[Envoy] HighConnectionErrors'
    annotations:
      description: Connection failure rate exceeding 10% for the past 15 minutes.
      summary: High connection errors on Envoy
    expr: irate(envoy_cluster_upstream_cx_connect_fail{job="envoy-sidecar"}[5m])
      > 0.1
    for: 15m
    labels:
      severity: warning
  - alert: '[Envoy] High5xxErrorRate'
    annotations:
      description: Error rate exceeding 2% for 5xx responses in the past 10 minutes.
      summary: High 5xx error rate on Envoy
    expr: irate(envoy_http_downstream_rq_xx{job="envoy-sidecar", envoy_response_code_class="5"}[5m])
      > 0.02
    for: 10m
    labels:
      severity: critical
  - alert: '[Envoy] EnvoyUptimeCheck'
    annotations:
      description: No 'up' metric reported for the past hour, indicating Envoy instance
        is not responsive.
      summary: Envoy instance is down
    expr: up{job="envoy-sidecar", draft !="draft-app"} == 0
    for: 1h
    labels:
      severity: critical

## Kubernetes rules

- name: k8s.rules
  rules:
  - expr: |-
      sum by (cluster, namespace, pod, container) (
        irate(container_cpu_usage_seconds_total{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}[5m])
      ) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (
        1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
      )
    record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate
  - expr: |-
      container_memory_working_set_bytes{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
      * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
        max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
      )
    record: node_namespace_pod_container:container_memory_working_set_bytes
  - expr: |-
      container_memory_rss{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
      * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
        max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
      )
    record: node_namespace_pod_container:container_memory_rss
  - expr: |-
      container_memory_cache{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
      * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
        max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
      )
    record: node_namespace_pod_container:container_memory_cache
  - expr: |-
      container_memory_swap{job="kubelet", metrics_path="/metrics/cadvisor", image!=""}
      * on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,
        max by(cluster, namespace, pod, node) (kube_pod_info{node!=""})
      )
    record: node_namespace_pod_container:container_memory_swap
  - expr: |-
      kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
      group_left() max by (namespace, pod, cluster) (
        (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
      )
    record: cluster:namespace:pod_memory:active:kube_pod_container_resource_requests
  - expr: |-
      sum by (namespace, cluster) (
          sum by (namespace, pod, cluster) (
              max by (namespace, pod, container, cluster) (
                kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}
              ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                kube_pod_status_phase{phase=~"Pending|Running"} == 1
              )
          )
      )
    record: namespace_memory:kube_pod_container_resource_requests:sum
  - expr: |-
      kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
      group_left() max by (namespace, pod, cluster) (
        (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
      )
    record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests
  - expr: |-
      sum by (namespace, cluster) (
          sum by (namespace, pod, cluster) (
              max by (namespace, pod, container, cluster) (
                kube_pod_container_resource_requests{resource="cpu",job="kube-state-metrics"}
              ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                kube_pod_status_phase{phase=~"Pending|Running"} == 1
              )
          )
      )
    record: namespace_cpu:kube_pod_container_resource_requests:sum
  - expr: |-
      kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}  * on (namespace, pod, cluster)
      group_left() max by (namespace, pod, cluster) (
        (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
      )
    record: cluster:namespace:pod_memory:active:kube_pod_container_resource_limits
  - expr: |-
      sum by (namespace, cluster) (
          sum by (namespace, pod, cluster) (
              max by (namespace, pod, container, cluster) (
                kube_pod_container_resource_limits{resource="memory",job="kube-state-metrics"}
              ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                kube_pod_status_phase{phase=~"Pending|Running"} == 1
              )
          )
      )
    record: namespace_memory:kube_pod_container_resource_limits:sum
  - expr: |-
      kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}  * on (namespace, pod, cluster)
      group_left() max by (namespace, pod, cluster) (
        (kube_pod_status_phase{phase=~"Pending|Running"} == 1)
        )
    record: cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits
  - expr: |-
      sum by (namespace, cluster) (
          sum by (namespace, pod, cluster) (
              max by (namespace, pod, container, cluster) (
                kube_pod_container_resource_limits{resource="cpu",job="kube-state-metrics"}
              ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (
                kube_pod_status_phase{phase=~"Pending|Running"} == 1
              )
          )
      )
    record: namespace_cpu:kube_pod_container_resource_limits:sum
  - expr: |-
      max by (cluster, namespace, workload, pod) (
        label_replace(
          label_replace(
            kube_pod_owner{job="kube-state-metrics", owner_kind="ReplicaSet"},
            "replicaset", "$1", "owner_name", "(.*)"
          ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (
            1, max by (replicaset, namespace, owner_name) (
              kube_replicaset_owner{job="kube-state-metrics"}
            )
          ),
          "workload", "$1", "owner_name", "(.*)"
        )
      )
    labels:
      workload_type: deployment
    record: namespace_workload_pod:kube_pod_owner:relabel
  - expr: |-
      max by (cluster, namespace, workload, pod) (
        label_replace(
          kube_pod_owner{job="kube-state-metrics", owner_kind="DaemonSet"},
          "workload", "$1", "owner_name", "(.*)"
        )
      )
    labels:
      workload_type: daemonset
    record: namespace_workload_pod:kube_pod_owner:relabel
  - expr: |-
      max by (cluster, namespace, workload, pod) (
        label_replace(
          kube_pod_owner{job="kube-state-metrics", owner_kind="StatefulSet"},
          "workload", "$1", "owner_name", "(.*)"
        )
      )
    labels:
      workload_type: statefulset
    record: namespace_workload_pod:kube_pod_owner:relabel
  - expr: |-
      max by (cluster, namespace, workload, pod) (
        label_replace(
          kube_pod_owner{job="kube-state-metrics", owner_kind="Job"},
          "workload", "$1", "owner_name", "(.*)"
        )
      )
    labels:
      workload_type: job
    record: namespace_workload_pod:kube_pod_owner:relabel

- name: kube-apiserver-availability.rules
  rules:
  - expr: avg_over_time(code_verb:apiserver_request_total:increase1h[30d]) * 24
      * 30
    record: code_verb:apiserver_request_total:increase30d
  - expr: sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"LIST|GET"})
    labels:
      verb: read
    record: code:apiserver_request_total:increase30d
  - expr: sum by (cluster, code) (code_verb:apiserver_request_total:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
    labels:
      verb: write
    record: code:apiserver_request_total:increase30d
  - expr: sum by (cluster, verb, scope) (increase(apiserver_request_slo_duration_seconds_count[1h]))
    record: cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase1h
  - expr: sum by (cluster, verb, scope) (avg_over_time(cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase1h[30d])
      * 24 * 30)
    record: cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d
  - expr: sum by (cluster, verb, scope, le) (increase(apiserver_request_slo_duration_seconds_bucket[1h]))
    record: cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase1h
  - expr: sum by (cluster, verb, scope, le) (avg_over_time(cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase1h[30d])
      * 24 * 30)
    record: cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d
  - expr: |-
      1 - (
        (
          # write too slow
          sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
          -
          sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
        ) +
        (
          # read too slow
          sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"LIST|GET"})
          -
          (
            (
              sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
              or
              vector(0)
            )
            +
            sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
            +
            sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
          )
        ) +
        # errors
        sum by (cluster) (code:apiserver_request_total:increase30d{code=~"5.."} or vector(0))
      )
      /
      sum by (cluster) (code:apiserver_request_total:increase30d)
    labels:
      verb: all
    record: apiserver_request:availability30d
  - expr: |-
      1 - (
        sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"LIST|GET"})
        -
        (
          # too slow
          (
            sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope=~"resource|",le="1"})
            or
            vector(0)
          )
          +
          sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="namespace",le="5"})
          +
          sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"LIST|GET",scope="cluster",le="30"})
        )
        +
        # errors
        sum by (cluster) (code:apiserver_request_total:increase30d{verb="read",code=~"5.."} or vector(0))
      )
      /
      sum by (cluster) (code:apiserver_request_total:increase30d{verb="read"})
    labels:
      verb: read
    record: apiserver_request:availability30d
  - expr: |-
      1 - (
        (
          # too slow
          sum by (cluster) (cluster_verb_scope:apiserver_request_slo_duration_seconds_count:increase30d{verb=~"POST|PUT|PATCH|DELETE"})
          -
          sum by (cluster) (cluster_verb_scope_le:apiserver_request_slo_duration_seconds_bucket:increase30d{verb=~"POST|PUT|PATCH|DELETE",le="1"})
        )
        +
        # errors
        sum by (cluster) (code:apiserver_request_total:increase30d{verb="write",code=~"5.."} or vector(0))
      )
      /
      sum by (cluster) (code:apiserver_request_total:increase30d{verb="write"})
    labels:
      verb: write
    record: apiserver_request:availability30d
  - expr: sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
    labels:
      verb: read
    record: code_resource:apiserver_request_total:rate5m
  - expr: sum by (cluster,code,resource) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
    labels:
      verb: write
    record: code_resource:apiserver_request_total:rate5m
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"2.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"3.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"4.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h
  - expr: sum by (cluster, code, verb) (increase(apiserver_request_total{job="apiserver",verb=~"LIST|GET|POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
    record: code_verb:apiserver_request_total:increase1h


- name: kube-apiserver-burnrate.rules
  rules:
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1d]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1d]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1d]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d]))
    labels:
      verb: read
    record: apiserver_request:burnrate1d
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[1h]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[1h]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[1h]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1h]))
    labels:
      verb: read
    record: apiserver_request:burnrate1h
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[2h]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[2h]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[2h]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[2h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[2h]))
    labels:
      verb: read
    record: apiserver_request:burnrate2h
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[30m]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[30m]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[30m]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[30m]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[30m]))
    labels:
      verb: read
    record: apiserver_request:burnrate30m
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[3d]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[3d]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[3d]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[3d]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[3d]))
    labels:
      verb: read
    record: apiserver_request:burnrate3d
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[5m]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[5m]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[5m]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[5m]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[5m]))
    labels:
      verb: read
    record: apiserver_request:burnrate5m
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
          -
          (
            (
              sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope=~"resource|",le="1"}[6h]))
              or
              vector(0)
            )
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="namespace",le="5"}[6h]))
            +
            sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward",scope="cluster",le="30"}[6h]))
          )
        )
        +
        # errors
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[6h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[6h]))
    labels:
      verb: read
    record: apiserver_request:burnrate6h
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1d]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1d]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1d]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1d]))
    labels:
      verb: write
    record: apiserver_request:burnrate1d
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[1h]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[1h]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[1h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[1h]))
    labels:
      verb: write
    record: apiserver_request:burnrate1h
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[2h]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[2h]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[2h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[2h]))
    labels:
      verb: write
    record: apiserver_request:burnrate2h
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[30m]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[30m]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[30m]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[30m]))
    labels:
      verb: write
    record: apiserver_request:burnrate30m
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[3d]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[3d]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[3d]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[3d]))
    labels:
      verb: write
    record: apiserver_request:burnrate3d
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[5m]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[5m]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[5m]))
    labels:
      verb: write
    record: apiserver_request:burnrate5m
  - expr: |-
      (
        (
          # too slow
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_count{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[6h]))
          -
          sum by (cluster) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward",le="1"}[6h]))
        )
        +
        sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",code=~"5.."}[6h]))
      )
      /
      sum by (cluster) (rate(apiserver_request_total{job="apiserver",verb=~"POST|PUT|PATCH|DELETE"}[6h]))
    labels:
      verb: write
    record: apiserver_request:burnrate6h

- name: kube-apiserver-histogram.rules
  rules:
  - expr: histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
    labels:
      quantile: "0.99"
      verb: read
    record: cluster_quantile:apiserver_request_slo_duration_seconds:histogram_quantile
  - expr: histogram_quantile(0.99, sum by (cluster, le, resource) (rate(apiserver_request_slo_duration_seconds_bucket{job="apiserver",verb=~"POST|PUT|PATCH|DELETE",subresource!~"proxy|attach|log|exec|portforward"}[5m]))) > 0
    labels:
      quantile: "0.99"
      verb: write
    record: cluster_quantile:apiserver_request_slo_duration_seconds:histogram_quantile

- name: kube-apiserver-slos
  rules:
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: |-
      sum(apiserver_request:burnrate1h) > (14.40 * 0.01000)
      and
      sum(apiserver_request:burnrate5m) > (14.40 * 0.01000)
    for: 2m
    labels:
      long: 1h
      severity: critical
      short: 5m
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: |-
      sum(apiserver_request:burnrate6h) > (6.00 * 0.01000)
      and
      sum(apiserver_request:burnrate30m) > (6.00 * 0.01000)
    for: 15m
    labels:
      long: 6h
      severity: critical
      short: 30m
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: |-
      sum(apiserver_request:burnrate1d) > (3.00 * 0.01000)
      and
      sum(apiserver_request:burnrate2h) > (3.00 * 0.01000)
    for: 1h
    labels:
      long: 1d
      severity: warning
      short: 2h
  - alert: KubeAPIErrorBudgetBurn
    annotations:
      description: The API server is burning too much error budget.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
      summary: The API server is burning too much error budget.
    expr: |-
      sum(apiserver_request:burnrate3d) > (1.00 * 0.01000)
      and
      sum(apiserver_request:burnrate6h) > (1.00 * 0.01000)
    for: 3h
    labels:
      long: 3d
      severity: warning
      short: 6h

- name: kube-prometheus-general.rules
  rules:
  - expr: count without(instance, pod, node) (up == 1)
    record: count:up1
  - expr: count without(instance, pod, node) (up == 0)
    record: count:up0

- name: kube-prometheus-node-recording.rules
  rules:
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[3m]))
      BY (instance)
    record: instance:node_cpu:rate:sum
  - expr: sum(rate(node_network_receive_bytes_total[3m])) BY (instance)
    record: instance:node_network_receive_bytes:rate:sum
  - expr: sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)
    record: instance:node_network_transmit_bytes:rate:sum
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
      WITHOUT (cpu, mode) / ON(instance) GROUP_LEFT() count(sum(node_cpu_seconds_total)
      BY (instance, cpu)) BY (instance)
    record: instance:node_cpu:ratio
  - expr: sum(rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal"}[5m]))
    record: cluster:node_cpu:sum_rate5m
  - expr: cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance,
      cpu))
    record: cluster:node_cpu:ratio

- name: kube-state-metrics
  rules:
  - alert: KubeStateMetricsListErrors
    annotations:
      description: kube-state-metrics is experiencing errors at an elevated rate
        in list operations. This is likely causing it to not be able to expose
        metrics about Kubernetes objects correctly or at all.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors
      summary: kube-state-metrics is experiencing errors in list operations.
    expr: |-
      (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m]))
        /
      sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])))
      > 0.01
    for: 15m
    labels:
      severity: critical
  - alert: KubeStateMetricsWatchErrors
    annotations:
      description: kube-state-metrics is experiencing errors at an elevated rate
        in watch operations. This is likely causing it to not be able to expose
        metrics about Kubernetes objects correctly or at all.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors
      summary: kube-state-metrics is experiencing errors in watch operations.
    expr: |-
      (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m]))
        /
      sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])))
      > 0.01
    for: 15m
    labels:
      severity: critical
  - alert: KubeStateMetricsShardingMismatch
    annotations:
      description: kube-state-metrics pods are running with different --total-shards
        configuration, some Kubernetes objects may be exposed multiple times or
        not exposed at all.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
      summary: kube-state-metrics sharding is misconfigured.
    expr: stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) !=
      0
    for: 15m
    labels:
      severity: critical
  - alert: KubeStateMetricsShardsMissing
    annotations:
      description: kube-state-metrics shards are missing, some Kubernetes objects
        are not being exposed.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
      summary: kube-state-metrics shards are missing.
    expr: |-
      2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) - 1
        -
      sum( 2 ^ max by (shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) )
      != 0
    for: 15m
    labels:
      severity: critical

### Kubernetes apps

- name: kubernetes-apps
  rules:
  - alert: "KubePodCrashLooping"
    annotations:
      message: "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is restarting {{ printf \"%.2f\" $value }} times / 5 minutes."
      "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepodcrashlooping"
    expr: |
      rate(kube_pod_container_status_restarts_total{job="kube-state-metrics"}[15m]) * 60 * 5 > 0
    for: "15m"
    labels:
      severity: "critical"
  - alert: KubePodNotReady
    annotations:
      description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a
        non-ready state for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
      summary: Pod has been in a non-ready state for more than 15 minutes.
    expr: |-
      sum by (namespace, pod, cluster) (
        max by(namespace, pod, cluster) (
          kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
        ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (
          1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
        )
      ) > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentGenerationMismatch
    annotations:
      description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has
        not been rolled back.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
      summary: Deployment generation mismatch due to possible roll-back
    expr: |-
      kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning
  - alert: KubeDeploymentReplicasMismatch
    annotations:
      description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }}
        has not matched the expected number of replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
      summary: Deployment has not matched the expected number of replicas.
    expr: |-
      (
        kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
          >
        kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
      ) and (
        changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetReplicasMismatch
    annotations:
      description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15
        minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
      summary: Deployment has not matched the expected number of replicas.
    expr: |-
      (
        kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
          !=
        kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
      ) and (
        changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetGenerationMismatch
    annotations:
      description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but
        has not been rolled back.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
      summary: StatefulSet generation mismatch due to possible roll-back
    expr: |-
      kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning
  - alert: KubeStatefulSetUpdateNotRolledOut
    annotations:
      description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
      summary: StatefulSet update has not been rolled out.
    expr: |-
      (
        max without (revision) (
          kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
            unless
          kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
        )
          *
        (
          kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
        )
      )  and (
        changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeDaemonSetRolloutStuck
    annotations:
      description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has
        not finished or progressed for at least 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
      summary: DaemonSet rollout is stuck.
    expr: |-
      (
        (
          kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        ) or (
          kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          0
        ) or (
          kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        ) or (
          kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
            !=
          kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        )
      ) and (
        changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
          ==
        0
      )
    for: 15m
    labels:
      severity: warning
  - alert: KubeContainerWaiting
    annotations:
      description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }}
        on container {{ $labels.container}} has been in waiting state for longer
        than 1 hour.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
      summary: Pod container waiting longer than 1 hour
    expr: sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",
      namespace=~".*"}) > 0
    for: 1h
    labels:
      severity: warning
  - alert: KubeDaemonSetNotScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
        $labels.daemonset }} are not scheduled.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
      summary: DaemonSet pods are not scheduled.
    expr: |-
      kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
        -
      kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
    for: 10m
    labels:
      severity: warning
  - alert: KubeDaemonSetMisScheduled
    annotations:
      description: '{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{
        $labels.daemonset }} are running where they are not supposed to run.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
      summary: DaemonSet pods are misscheduled.
    expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics",
      namespace=~".*"} > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeJobNotCompleted
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking
        more than {{ "43200" | humanizeDuration }} to complete.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
      summary: Job did not complete in time
    expr: |-
      time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{job="kube-state-metrics", namespace=~".*"}
        and
      kube_job_status_active{job="kube-state-metrics", namespace=~".*"} > 0) > 43200
    labels:
      severity: warning
  - alert: KubeJobFailed
    annotations:
      description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to
        complete. Removing failed job after investigation should clear this alert.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
      summary: Job failed to complete.
    expr: kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeHpaReplicasMismatch
    annotations:
      description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
        has not matched the desired number of replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
      summary: HPA has not matched desired number of replicas.
    expr: |-
      (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~".*"}
        !=
      kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        >
      kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        <
      kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"})
        and
      changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}[15m]) == 0
    for: 15m
    labels:
      severity: warning
  - alert: KubeHpaMaxedOut
    annotations:
      description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }}
        has been running at max replicas for longer than 15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
      summary: HPA is running at max replicas
    expr: |-
      kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
        ==
      kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"}
    for: 15m
    labels:
      severity: warning


### Kubernetes storage

- name: kubernetes-storage
  rules:
  - alert: "KubePersistentVolumeUsageCritical"
    annotations:
      message: "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free."
      "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumeusagecritical"
    expr: |
      kubelet_volume_stats_available_bytes{job="kubelet"}
        /
      kubelet_volume_stats_capacity_bytes{job="kubelet"}
        < 0.03
    for: "1m"
    labels:
      severity: "critical"
  - alert: "KubePersistentVolumeFullInFourDays"
    annotations:
      message: "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available."
      "runbook_url": "https://github.com/kubernetes-monitoring/kubernetes-mixin/tree/master/runbook.md#alert-name-kubepersistentvolumefullinfourdays"
    expr: |
      (
        kubelet_volume_stats_available_bytes{job="kubelet"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet"}
      ) < 0.15
      and
      predict_linear(kubelet_volume_stats_available_bytes{job="kubelet"}[6h], 4 * 24 * 3600) < 0
    for: "1h"
    labels:
      severity: "critical"  
  - alert: KubePersistentVolumeFillingUp
    annotations:
      description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} is only {{ $value | humanizePercentage }} free.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
      summary: PersistentVolume is filling up.
    expr: |-
      (
        kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.03
      and
      kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      unless on(namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on(namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1m
    labels:
      severity: critical
  - alert: KubePersistentVolumeFillingUp
    annotations:
      description: Based on recent sampling, the PersistentVolume claimed by {{
        $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }}
        is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
      summary: PersistentVolume is filling up.
    expr: |-
      (
        kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
          /
        kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
      ) < 0.15
      and
      kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
      and
      predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
      unless on(namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
      unless on(namespace, persistentvolumeclaim)
      kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
    for: 1h
    labels:
      severity: warning
  - alert: KubePersistentVolumeErrors
    annotations:
      description: The persistent volume {{ $labels.persistentvolume }} has status {{ $labels.phase }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
      summary: PersistentVolume is having issues with provisioning.
    expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
    for: 5m
    labels:
      severity: critical

### Kubernetes system

- name: kubernetes-system
  rules:
  - alert: KubeVersionMismatch
    annotations:
      description: There are {{ $value }} different semantic versions of Kubernetes
        components running.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
      summary: Different semantic versions of Kubernetes components running.
    expr: count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) > 1
    for: 15m
    labels:
      severity: warning
  - alert: KubeClientErrors
    annotations:
      description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors
      summary: Kubernetes API server client is experiencing errors.
    expr: |-
      (sum(rate(rest_client_requests_total{code=~"5.."}[5m])) by (cluster, instance, job, namespace)
        /
      sum(rate(rest_client_requests_total[5m])) by (cluster, instance, job, namespace))
      > 0.01
    for: 15m
    labels:
      severity: warning

### Node-Exporter

- name: node-exporter
  rules:
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available space left and is filling
        up.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
      summary: Filesystem is predicted to run out of space within the next 24
        hours.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemSpaceFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available space left and is filling
        up fast.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
      summary: Filesystem is predicted to run out of space within the next 4 hours.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 10
      and
        predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available space left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
      summary: Filesystem has less than 5% space left.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 30m
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfSpace
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available space left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
      summary: Filesystem has less than 3% space left.
    expr: |-
      (
        node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 30m
    labels:
      severity: critical
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available inodes left and is filling
        up.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
      summary: Filesystem is predicted to run out of inodes within the next 24
        hours.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 40
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemFilesFillingUp
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available inodes left and is filling
        up fast.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
      summary: Filesystem is predicted to run out of inodes within the next 4
        hours.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 20
      and
        predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available inodes left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
      summary: Filesystem has less than 5% inodes left.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: warning
  - alert: NodeFilesystemAlmostOutOfFiles
    annotations:
      description: Filesystem on {{ $labels.device }} at {{ $labels.instance }}
        has only {{ printf "%.2f" $value }}% available inodes left.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
      summary: Filesystem has less than 3% inodes left.
    expr: |-
      (
        node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
      and
        node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
      )
    for: 1h
    labels:
      severity: critical
  - alert: NodeNetworkReceiveErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has
        encountered {{ printf "%.0f" $value }} receive errors in the last two
        minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
      summary: Network interface is reporting many receive errors.
    expr: rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeNetworkTransmitErrs
    annotations:
      description: '{{ $labels.instance }} interface {{ $labels.device }} has
        encountered {{ printf "%.0f" $value }} transmit errors in the last two
        minutes.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
      summary: Network interface is reporting many transmit errors.
    expr: rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01
    for: 1h
    labels:
      severity: warning
  - alert: NodeHighNumberConntrackEntriesUsed
    annotations:
      description: '{{ $value | humanizePercentage }} of conntrack entries are
        used.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
      summary: Number of conntrack are getting close to the limit.
    expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) > 0.75
    labels:
      severity: warning
  - alert: NodeTextFileCollectorScrapeError
    annotations:
      description: Node Exporter text file collector failed to scrape.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
      summary: Node Exporter text file collector failed to scrape.
    expr: node_textfile_scrape_error{job="node-exporter"} == 1
    labels:
      severity: warning
  - alert: NodeClockSkewDetected
    annotations:
      description: Clock on {{ $labels.instance }} is out of sync by more than
        300s. Ensure NTP is configured correctly on this host.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
      summary: Clock skew detected.
    expr: |-
      (
        node_timex_offset_seconds{job="node-exporter"} > 0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
      )
      or
      (
        node_timex_offset_seconds{job="node-exporter"} < -0.05
      and
        deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
      )
    for: 10m
    labels:
      severity: warning
  - alert: NodeClockNotSynchronising
    annotations:
      description: Clock on {{ $labels.instance }} is not synchronising. Ensure
        NTP is configured on this host.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
      summary: Clock not synchronising.
    expr: |-
      min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
      and
      node_timex_maxerror_seconds{job="node-exporter"} >= 16
    for: 10m
    labels:
      severity: warning
  - alert: NodeRAIDDegraded
    annotations:
      description: RAID array '{{ $labels.device }}' on {{ $labels.instance }}
        is in degraded state due to one or more disks failures. Number of spare
        drives is insufficient to fix issue automatically.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
      summary: RAID Array is degraded
    expr: node_md_disks_required{job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
      - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}) > 0
    for: 15m
    labels:
      severity: critical
  - alert: NodeRAIDDiskFailure
    annotations:
      description: At least one device in RAID array on {{ $labels.instance }}
        failed. Array '{{ $labels.device }}' needs attention and possibly a disk
        swap.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
      summary: Failed device in RAID array
    expr: node_md_disks{state="failed",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} > 0
    labels:
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently
        at {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |-
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
      )
    for: 15m
    labels:
      severity: warning
  - alert: NodeFileDescriptorLimit
    annotations:
      description: File descriptors limit at {{ $labels.instance }} is currently
        at {{ printf "%.2f" $value }}%.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
      summary: Kernel is predicted to exhaust file descriptors limit soon.
    expr: |-
      (
        node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
      )
    for: 15m
    labels:
      severity: critical

- name: node-exporter.rules
  rules:
  - expr: |-
      count without (cpu, mode) (
        node_cpu_seconds_total{job="node-exporter",mode="idle"}
      )
    record: instance:node_num_cpu:sum
  - expr: |-
      1 - avg without (cpu) (
        sum without (mode) (rate(node_cpu_seconds_total{job="node-exporter", mode=~"idle|iowait|steal"}[5m]))
      )
    record: instance:node_cpu_utilisation:rate5m
  - expr: |-
      (
        node_load1{job="node-exporter"}
      /
        instance:node_num_cpu:sum{job="node-exporter"}
      )
    record: instance:node_load1_per_cpu:ratio
  - expr: |-
      1 - (
        (
          node_memory_MemAvailable_bytes{job="node-exporter"}
          or
          (
            node_memory_Buffers_bytes{job="node-exporter"}
            +
            node_memory_Cached_bytes{job="node-exporter"}
            +
            node_memory_MemFree_bytes{job="node-exporter"}
            +
            node_memory_Slab_bytes{job="node-exporter"}
          )
        )
      /
        node_memory_MemTotal_bytes{job="node-exporter"}
      )
    record: instance:node_memory_utilisation:ratio
  - expr: irate(node_vmstat_pgmajfault{job="node-exporter"}[5m])
    record: instance:node_vmstat_pgmajfault:rate5m
  - expr: rate(node_disk_io_time_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
    record: instance_device:node_disk_io_time_seconds:rate5m
  - expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m])
    record: instance_device:node_disk_io_time_weighted_seconds:rate5m
  - expr: |-
      sum without (device) (
        rate(node_network_receive_bytes_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_receive_bytes_excluding_lo:rate5m
  - expr: |-
      sum without (device) (
        rate(node_network_transmit_bytes_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_transmit_bytes_excluding_lo:rate5m
  - expr: |-
      sum without (device) (
        rate(node_network_receive_drop_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_receive_drop_excluding_lo:rate5m
  - expr: |-
      sum without (device) (
        rate(node_network_transmit_drop_total{job="node-exporter", device!="lo"}[5m])
      )
    record: instance:node_network_transmit_drop_excluding_lo:rate5m

- name: node-network
  rules:
  - alert: NodeNetworkInterfaceFlapping
    annotations:
      description: Network interface "{{ $labels.device }}" changing its up status
        often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
      summary: Network interface is often changing its status
    expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) >
      2
    for: 2m
    labels:
      severity: warning

- name: node.rules
  rules:
  - expr: |-
      topk by(cluster, namespace, pod) (1,
        max by (cluster, node, namespace, pod) (
          label_replace(kube_pod_info{job="kube-state-metrics",node!=""}, "pod", "$1", "pod", "(.*)")
      ))
    record: 'node_namespace_pod:kube_pod_info:'
  - expr: |-
      count by (cluster, node) (
        node_cpu_seconds_total{mode="idle",job="node-exporter"}
        * on (namespace, pod) group_left(node)
        topk by(namespace, pod) (1, node_namespace_pod:kube_pod_info:)
      )
    record: node:node_num_cpu:sum
  - expr: |-
      sum(
        node_memory_MemAvailable_bytes{job="node-exporter"} or
        (
          node_memory_Buffers_bytes{job="node-exporter"} +
          node_memory_Cached_bytes{job="node-exporter"} +
          node_memory_MemFree_bytes{job="node-exporter"} +
          node_memory_Slab_bytes{job="node-exporter"}
        )
      ) by (cluster)
    record: :node_memory_MemAvailable_bytes:sum
  - expr: |-
      avg by (cluster, node) (
        sum without (mode) (
          rate(node_cpu_seconds_total{mode!="idle",mode!="iowait",mode!="steal",job="node-exporter"}[5m])
        )
      )
    record: node:node_cpu_utilization:ratio_rate5m
  - expr: |-
      avg by (cluster) (
        node:node_cpu_utilization:ratio_rate5m
      )
    record: cluster:node_cpu:ratio_rate5m

### Prometheus rules

- name: prometheus
  rules:
  - alert: PrometheusBadConfig
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed
        to reload its configuration.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig
      summary: Failed Prometheus configuration reload.
    expr: |-
      # Without max_over_time, failed scrapes could create false negatives, see
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
      max_over_time(prometheus_config_last_reload_successful{job="kube-prometheus-stack-prometheus"}[5m]) == 0
    for: 10m
    labels:
      severity: critical
  - alert: PrometheusNotificationQueueRunningFull
    annotations:
      description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}}
        is running full.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull
      summary: Prometheus alert notification queue predicted to run full in less
        than 30m.
    expr: |-
      # Without min_over_time, failed scrapes could create false negatives, see
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
      (
        predict_linear(prometheus_notifications_queue_length{job="kube-prometheus-stack-prometheus"}[5m], 60 * 30)
      >
        min_over_time(prometheus_notifications_queue_capacity{job="kube-prometheus-stack-prometheus"}[5m])
      )
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
    annotations:
      description: '{{ printf "%.1f" $value }}% errors while sending alerts from
        Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
      summary: Prometheus has encountered more than 1% errors sending alerts to
        a specific Alertmanager.
    expr: |-
      (
        rate(prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus"}[5m])
      /
        rate(prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus"}[5m])
      )
      * 100
      > 1
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusNotConnectedToAlertmanagers
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected
        to any Alertmanagers.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
      summary: Prometheus is not connected to any Alertmanagers.
    expr: |-
      # Without max_over_time, failed scrapes could create false negatives, see
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
      max_over_time(prometheus_notifications_alertmanagers_discovered{job="kube-prometheus-stack-prometheus", namespace="prometheus-monitoring-prod", namespace="prometheus-monitoring-reg-prod"}[5m]) < 1
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusTSDBReloadsFailing
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected
        {{$value | humanize}} reload failures over the last 3h.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing
      summary: Prometheus has issues reloading blocks from disk.
    expr: increase(prometheus_tsdb_reloads_failures_total{job="kube-prometheus-stack-prometheus"}[3h]) > 0
    for: 4h
    labels:
      severity: warning
  - alert: PrometheusTSDBCompactionsFailing
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing
      summary: Prometheus has issues compacting blocks.
    expr: increase(prometheus_tsdb_compactions_failed_total{job="kube-prometheus-stack-prometheus"}[3h]) > 0
    for: 4h
    labels:
      severity: warning
  - alert: PrometheusNotIngestingSamples
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting
        samples.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples
      summary: Prometheus is not ingesting samples.
    expr: |-
      (
        rate(prometheus_tsdb_head_samples_appended_total{job="kube-prometheus-stack-prometheus"}[5m]) <= 0
      and
        (
          sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="kube-prometheus-stack-prometheus"}) > 0
        or
          sum without(rule_group) (prometheus_rule_group_rules{job="kube-prometheus-stack-prometheus"}) > 0
        )
      )
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusDuplicateTimestamps
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated
        timestamp.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps
      summary: Prometheus is dropping samples with duplicate timestamps.
    expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="kube-prometheus-stack-prometheus"}[5m]) > 1
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusOutOfOrderTimestamps
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of
        order.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps
      summary: Prometheus drops samples with out-of-order timestamps.
    expr: rate(prometheus_target_scrapes_sample_out_of_order_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusRemoteStorageFailures
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to
        send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{
        $labels.url }}
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures
      summary: Prometheus fails to send samples to remote storage.
    expr: |-
      (
        (rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus"}[5m]))
      /
        (
          (rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus"}[5m]))
        +
          (rate(prometheus_remote_storage_succeeded_samples_total{job="kube-prometheus-stack-prometheus"}[5m]) or rate(prometheus_remote_storage_samples_total{job="kube-prometheus-stack-prometheus"}[5m]))
        )
      )
      * 100
      > 1
    for: 15m
    labels:
      severity: critical
  - alert: PrometheusRemoteWriteBehind
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
        is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{
        $labels.url }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind
      summary: Prometheus remote write is behind.
    expr: |-
      # Without max_over_time, failed scrapes could create false negatives, see
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
      (
        max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="kube-prometheus-stack-prometheus"}[5m])
      - ignoring(remote_name, url) group_right
        max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="kube-prometheus-stack-prometheus"}[5m])
      )
      > 120
    for: 15m
    labels:
      severity: critical
  - alert: PrometheusRemoteWriteDesiredShards
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write
        desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max
        of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="kube-prometheus-stack-prometheus"}`
        $labels.instance | query | first | value }}.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards
      summary: Prometheus remote write desired shards calculation wants to run
        more than configured max shards.
    expr: |-
      # Without max_over_time, failed scrapes could create false negatives, see
      # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
      (
        max_over_time(prometheus_remote_storage_shards_desired{job="kube-prometheus-stack-prometheus"}[5m])
      >
        max_over_time(prometheus_remote_storage_shards_max{job="kube-prometheus-stack-prometheus"}[5m])
      )
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusMissingRuleEvaluations
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations
      summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
    expr: increase(prometheus_rule_group_iterations_missed_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusTargetLimitHit
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded
        the configured target_limit.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit
      summary: Prometheus has dropped targets because some scrape configs have
        exceeded the targets limit.
    expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusLabelLimitHit
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured
        label_limit, label_name_length_limit or label_value_length_limit.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit
      summary: Prometheus has dropped targets because some scrape configs have
        exceeded the labels limit.
    expr: increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusScrapeBodySizeLimitHit
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets
        exceeded the configured body_size_limit.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit
      summary: Prometheus has dropped some targets that exceeded body size limit.
    expr: increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusScrapeSampleLimitHit
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $value }} scrapes in the last 5m because some targets
        exceeded the configured sample_limit.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit
      summary: Prometheus has failed scrapes that have exceeded the configured
        sample limit.
    expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total{job="kube-prometheus-stack-prometheus"}[5m]) > 0
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusTargetSyncFailure
    annotations:
      description: '{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}}
        have failed to sync because invalid configuration was supplied.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure
      summary: Prometheus has failed to sync targets.
    expr: increase(prometheus_target_sync_failed_total{job="kube-prometheus-stack-prometheus"}[30m]) > 0
    for: 5m
    labels:
      severity: critical
  - alert: PrometheusHighQueryLoad
    annotations:
      description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API
        has less than 20% available capacity in its query engine for the last
        15 minutes.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload
      summary: Prometheus is reaching its maximum capacity serving concurrent
        requests.
    expr: avg_over_time(prometheus_engine_queries{job="kube-prometheus-stack-prometheus"}[5m])
      / max_over_time(prometheus_engine_queries_concurrent_max{job="kube-prometheus-stack-prometheus"}[5m]) > 0.8
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
    annotations:
      description: '{{ printf "%.1f" $value }}% minimum errors while sending alerts
        from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
      summary: Prometheus encounters more than 3% errors sending alerts to any
        Alertmanager.
    expr: |-
      min without (alertmanager) (
        rate(prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus",alertmanager!~``}[5m])
      /
        rate(prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus",alertmanager!~``}[5m])
      )
      * 100
      > 3
    for: 15m
    labels:
      severity: critical

  - alert: PrometheusOperatorListErrors
    annotations:
      description: Errors while performing List operations in controller {{$labels.controller}}
        in {{$labels.namespace}} namespace.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors
      summary: Errors while performing list operations in controller.
    expr: (sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_failed_total{job="kube-prometheus-stack-operator"}[10m]))
      / sum by (controller,namespace,cluster) (rate(prometheus_operator_list_operations_total{job="kube-prometheus-stack-operator"}[10m]))) > 0.4
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusOperatorWatchErrors
    annotations:
      description: Errors while performing watch operations in controller {{$labels.controller}}
        in {{$labels.namespace}} namespace.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors
      summary: Errors while performing watch operations in controller.
    expr: (sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_failed_total{job="kube-prometheus-stack-operator"}[5m]))
      / sum by (controller,namespace,cluster) (rate(prometheus_operator_watch_operations_total{job="kube-prometheus-stack-operator"}[5m]))) > 0.4
    for: 15m
    labels:
      severity: warning
  - alert: PrometheusOperatorSyncFailed
    annotations:
      description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed
      summary: Last controller reconciliation failed
    expr: min_over_time(prometheus_operator_syncs{status="failed",job="kube-prometheus-stack-operator"}[5m]) > 0
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusOperatorReconcileErrors
    annotations:
      description: '{{ $value | humanizePercentage }} of reconciling operations
        failed for {{ $labels.controller }} controller in {{ $labels.namespace
        }} namespace.'
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
      summary: Errors while reconciling controller.
    expr: (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_errors_total{job="kube-prometheus-stack-operator"}[5m])))
      / (sum by (controller,namespace,cluster) (rate(prometheus_operator_reconcile_operations_total{job="kube-prometheus-stack-operator"}[5m]))) > 0.1
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusOperatorNodeLookupErrors
    annotations:
      description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
      summary: Errors while reconciling Prometheus.
    expr: rate(prometheus_operator_node_address_lookup_errors_total{job="kube-prometheus-stack-operator"}[5m]) > 0.1
    for: 10m
    labels:
      severity: warning
  - alert: PrometheusOperatorNotReady
    annotations:
      description: Prometheus operator in {{ $labels.namespace }} namespace isn't
        ready to reconcile {{ $labels.controller }} resources.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready
      summary: Prometheus operator not ready
    expr: min by (controller,namespace,cluster) (max_over_time(prometheus_operator_ready{job="kube-prometheus-stack-operator"}[5m])
      == 0)
    for: 5m
    labels:
      severity: warning
  - alert: PrometheusOperatorRejectedResources
    annotations:
      description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
      runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources
      summary: Resources rejected by Prometheus operator
    expr: min_over_time(prometheus_operator_managed_resources{state="rejected",job="kube-prometheus-stack-operator"}[5m]) > 0
    for: 5m
    labels:
      severity: warning

###Keycloak
- name: Keycloak
  rules:
  - alert: '[Keycloak] Number of 4xx and 5xx responses high last 30 minutes'
    annotations:
      description: Number of failed request is increasing
    expr: |
      sum by (code)(irate(keycloak_response_errors[30m])) > 3
    for: 10m
    labels:
      severity: major
  - alert: '[Keycloak] High usage heap memory> 75%'
    annotations:
      description: Heap memory usage for container keycloack > 75%
    expr: |
      sum(jvm_memory_bytes_used{container="keycloak", area="heap"})*100/sum(jvm_memory_bytes_max{container="keycloak", area="heap"}) > 75
    for: 10m
    labels:
      severity: major
  - alert: '[Keycloak] High usage  memory> 75%'
    annotations:
      description: Memory usage for container keycloack > 75%
    expr: |
      sum(jvm_memory_bytes_used{container="keycloak", area="nonheap"})*100/sum(jvm_memory_bytes_max{container="keycloak", area="nonheap"}) > 75
    for: 10m
    labels:
      severity: major
  - alert: '[Keycloak] Percentage of GET request served in 100ms or less. Is less
      that 75%'
    annotations:
      description: Number of slow  GET request is high
    expr: |
      sum(rate(keycloak_request_duration_bucket{method="GET", le="100.0"}[30m])) / sum(rate(keycloak_request_duration_count{method="GET"}[30m])) * 100 < 25
    for: 10m
    labels:
      severity: major
  - alert: '[Keycloak] Percentage of POST request served in 100ms or less. Is
      less that 75%'
    annotations:
      description: Number of slow  POST request is high
    expr: |
      sum(rate(keycloak_request_duration_bucket{method="POST", le="100.0"}[30m])) / sum(rate(keycloak_request_duration_count{method="POST"}[30m])) * 100 < 25
    for: 10m
    labels:
      severity: major

### RabbitMQ
- name: RabbitMQ
  rules:
  - alert: '[RabbitMQ] Low Disk Watermark Predicted'
    annotations:
      description: The predicted free disk space in 24 hours from now is low
    expr: |
      ( predict_linear(rabbitmq_disk_space_available_bytes[24h], 60*60*24) < rabbitmq_disk_space_available_limit_bytes )and( count_over_time(rabbitmq_disk_space_available_limit_bytes[2h] offset 22h) > 0)
    for: 60m
    labels:
      severity: warning
  - alert: '[RabbitMQ] High Connection Churn'
    annotations:
      description: There are a high connection churn
    expr: |
      ( sum(rate(rabbitmq_connections_closed_total[5m]) )  + sum(rate(rabbitmq_connections_opened_total[5m]) ) )/sum (rabbitmq_connections) > 0.1 unless sum (rabbitmq_connections) < 100
    for: 10m
    labels:
      severity: warning
  - alert: '[RabbitMQ] Unroutable Messages'
    annotations:
      description: There were unroutable message within the last 5 minutes in
        RabbitMQ cluster
    expr: |
      sum (increase(rabbitmq_channel_messages_unroutable_dropped_total[5m])) >= 1 or sum (increase(rabbitmq_channel_messages_unroutable_returned_total[5m])) >= 1
    for: 5m
    labels:
      severity: warning
  - alert: '[RabbitMQ] File Descriptors Near Limit'
    annotations:
      description: The file descriptors are near to the limit
    expr: |
      sum (max_over_time(rabbitmq_process_open_fds[5m]))/sum (rabbitmq_process_max_tcp_sockets)> 0.8
    for: 10m
    labels:
      severity: warning
  - alert: '[RabbitMQ] TCP Sockets Near Limit'
    annotations:
      description: The TCP sockets are near to the limit
    expr: |
      sum (max_over_time(rabbitmq_process_open_tcp_sockets[5m]))/sum (rabbitmq_process_max_tcp_sockets)> 0.8
    for: 10m
    labels:
      severity: warning
  - alert: IN TEST [RabbitMQ] RabbitMQ messages queue ready to be sent is to High
    annotations:
      description: Messages in queue with status ready is higher than expected,  please
        investigate
      responsible: DevOPS
    expr: |
      sum(rabbitmq_queue_messages_ready) > 20000
    for: 5m
    labels:
      severity: warning
  - alert: IN TEST [RabbitMQ] RabbitMQ queued messages not acknowledged
    annotations:
      description: Messages in queue with and not acknowledged is higher than
        expected,  please investigate
      responsible: DevOPS
    expr: |
      sum(rabbitmq_queue_messages_unacked) > 100
    for: 5m
    labels:
      severity: warning
  - alert: IN TEST [RabbitMQ] RabbitMQ queued messages not acknowledged
    annotations:
      description: Messages in queue with and not acknowledged is higher than
        expected,  please investigate
      responsible: DevOPS
    expr: |
      sum(rabbitmq_queue_messages_unacked) > 100
    for: 5m
    labels:
      severity: warning
  - alert: '[RabbitMQ] RabbitMQ unexpected increase of received messages more
      than 10000 in 3 seconds '
    annotations:
      description: Number of messages received for 3 seconds is higher than  10000,  please
        investigate
      responsible: DevOPS
    expr: |
      sum(increase(rabbitmq_global_messages_received_total[3s])) > 10000
    for: 5m
    labels:
      severity: warning


### Kong API
- name: KONG Api
  rules:
  - alert: '[Kong API] Lot of requests are in waiting phase'
    annotations:
      title: 'Instance {{ $labels.instance }} down'
      description: KONG API  Lot of requests are in waiting phase
    expr: |
      kong_nginx_http_current_connections{state="waiting"} > 10
    for: 10m
    labels:
      severity: major


### MySQL Account api
- name: MySQL Account api
  rules:
  - alert: '[Common account api] Authorization manager fails with status 500'
    annotations:
      title: 'Common Account API - Authorization manager 500 errors'
      description: 'Auth manager reporting status 500'
    expr: |
      increase(http_requests_received_total{job="authorisation-manager", code=~"5.*",controller="Authorize"}[1m]) > 10
    for: 5m
    labels:
      severity: critical
  
  - alert: '[MYSQL account api] Mysql Down'
    annotations:
      title: 'MySQL Account API - MySQL Down'
      description: 'MySQL instance is down'
    expr: |
      mysql_up == 0
    for: 5m
    labels:
      severity: critical
  
  - alert: '[MYSQL account api] Mysql Restarted'
    annotations:
      title: 'MySQL Account API - MySQL Restarted'
      description: 'MySQL has just been restarted, less than one minute ago'
      responsible: 'OPS'
      summary: 'MySQL restarted'
    expr: |
      mysql_global_status_uptime < 60
    for: 5m
    labels:
      severity: info
  
  - alert: '[MYSQL account api] Mysql Too many Connections (>80%)'
    annotations:
      title: 'MySQL Account API - Too many connections'
      description: 'More than 80% of MySQL connections are in use'
    expr: |
      avg by (instance) (mysql_global_status_threads_connected) / avg by (instance) (mysql_global_variables_max_connections) * 100 > 80
    for: 2m
    labels:
      severity: warning
  
  - alert: '[MYSQL account api] Mysql High Threads Running'
    annotations:
      title: 'MySQL Account API - High threads running'
      description: 'More than 60% of MySQL connections are in running state'
    expr: |
      avg by (instance) (mysql_global_status_threads_running) / avg by (instance) (mysql_global_variables_max_connections) * 100 > 60
    for: 2m
    labels:
      severity: warning
  
  - alert: '[MYSQL account api] Mysql High Open Files'
    annotations:
      title: 'MySQL Account API - High open files'
      description: 'More than 80% of MySQL files open'
    expr: |
      avg by (instance) (mysql_global_variables_innodb_open_files) / avg by (instance)(mysql_global_variables_open_files_limit) * 100 > 80
    for: 2m
    labels:
      severity: warning
  
  - alert: '[MYSQL account api] Mysql Innodb Log Waits'
    annotations:
      title: 'MySQL Account API - Innodb log waits'
      description: 'MySQL innodb log writes stalling'
    expr: |
      irate(mysql_global_status_innodb_log_waits[15m]) > 10
    for: 5m
    labels:
      severity: warning
      metric_type: 'counter'  # Documenting the metric type
  
  - alert: '[MYSQL account api] Mysql Slave Io Thread Not Running'
    annotations:
      title: 'MySQL Account API - Slave IO thread not running'
      description: 'MySQL Slave IO thread not running'
    expr: |
      mysql_slave_status_master_server_id > 0 and ON (instance) mysql_slave_status_slave_io_running == 0
    for: 5m
    labels:
      severity: critical
  
  - alert: '[MYSQL account api] Mysql Slave Sql Thread Not Running'
    annotations:
      title: 'MySQL Account API - Slave SQL thread not running'
      description: 'MySQL Slave SQL thread not running'
    expr: |
      mysql_slave_status_master_server_id > 0 and ON (instance) mysql_slave_status_slave_sql_running == 0
    for: 5m
    labels:
      severity: critical
  
  - alert: '[MYSQL account api] Mysql Slave Replication Lag'
    annotations:
      title: 'MySQL Account API - Slave replication lag'
      description: 'MySQL Slave replication lag'
    expr: |
      mysql_slave_status_master_server_id > 0 and ON (instance) (mysql_slave_status_seconds_behind_master - mysql_slave_status_sql_delay) > 30
    for: 1m
    labels:
      severity: critical


###  Redis Account API
- name: Redis Account API
  rules:
  - alert: '[Redis Account API] Low UpTime'
    annotations:
      description: Uptime of less than 1 hour in a redis instance
    expr: |
      redis_uptime_in_seconds < 3600
    for: 15m
    labels:
      severity: critical
  - alert: '[Redis Account API] High Clients Usage'
    annotations:
      description: High client connections usage
    expr: |
      (redis_connected_clients /  redis_config_maxclients) > 0.85
    for: 15m
    labels:
      severity: critical
  - alert: '[Redis Account API] High Response Time'
    annotations:
      description: Response time over 250ms
    expr: |
      (sum  (rate(redis_commands_duration_seconds_total[5m])) / sum  (rate(redis_commands_processed_total[5m]))) > 0.250
    for: 10m
    labels:
      severity: critical
  - alert: '[Redis Account API] High Keys Eviction Ratio'
    annotations:
      description: High keys eviction ratio
    expr: |
      (sum  (rate(redis_evicted_keys_total[5m])) /  sum  (redis_db_keys)) > 0.1
    for: 30m
    labels:
      severity: critical
  - alert: '[Redis Account API] Recurrent Rejected Connections'
    annotations:
      description: Recurrent rejected connections
    expr: |
      (rate(redis_rejected_connections_total[5m])) > 0
    for: 15m
    labels:
      severity: critical
  - alert: '[Redis Account API] Low Hit Ratio'
    annotations:
      description: Low keyspace hit ratio
    expr: |
      (rate(redis_keyspace_hits_total[5m]) / ( rate(redis_keyspace_misses_total[5m]) + rate(redis_keyspace_hits_total[5m]) )) < 0.9
    for: 30m
    labels:
      severity: warning
  - alert: '[Redis Account API] Exporter Process Down'
    annotations:
      description: Exporter proccess is not serving metrics
    expr: |
      absent(redis_uptime_in_seconds) > 0
    for: 30m
    labels:
      severity: warning
  - alert: '[Redis Account API] Redis pods restarts to often last 5 minutes'
    annotations:
      description: One or more Redis containers were restarted too often within
        the last 5 minutes
      summary: One or more Redis containers restarted too often
    expr: |
      count(count_over_time(container_last_seen{container=~".*redis",pod=~".*redis-[0-9]"}[5m])) > 2 * count(container_last_seen{container=~".*redis",pod=~".*redis-[0-9]"})
    for: 5m
    labels:
      severity: warning

### PostgreSQL
- name: PostgreSQL
  rules:
  - alert: '[PostgreSQL] Instance Down'
    annotations:
      description: PostgreSQL instance is unavailable
    expr: |
      pg_up == 0
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] Low UpTime'
    annotations:
      description: The PostgreSQL instance has a UpTime of less than 1 hour
    expr: |
      (time() - pg_postmaster_start_time_seconds)< 3600
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] Max Write Buffer Reached'
    annotations:
      description: Background writer stops because it reached the maximum write
        buffers
    expr: |
      irate(pg_stat_bgwriter_maxwritten_clean[5m]) > 0
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] High WAL Files Archive Error Rate'
    annotations:
      description: High error rate in WAL files archiver
    expr: |
      rate(pg_stat_archiver_failed_count[5m]) /(rate(pg_stat_archiver_archived_count[5m]) + rate(pg_stat_archiver_failed_count[5m]))> 0.1
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] Low Available Connections'
    annotations:
      description: Low available network connections
    expr: |
      sum  (pg_stat_activity_count)/( sum  (pg_settings_max_connections) - sum  (pg_settings_superuser_reserved_connections)) > 0.85
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] High Response Time'
    annotations:
      description: High response time in at least one of the databases
    expr: |
      avg by (datname) (irate(pg_stat_activity_max_tx_duration{datname!~'template.*'}[5m]))> (2 * 60)
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] Low Cache Hit Rate'
    annotations:
      description: Low cache hit rate
    expr: |
      avg  (irate(pg_stat_database_blks_hit{datname!~'template.*'}[5m]) / (irate(pg_stat_database_blks_hit{datname!~'template.*'}[5m]) + irate(pg_stat_database_blks_read{datname!~'template.*'}[5m]) )) < 0.9
    for: 10m
    labels:
      severity: critical
  - alert: '[PostgreSQL] DeadLocks In Database'
    annotations:
      description: Deadlocks detected in database
    expr: |
      irate(pg_stat_database_deadlocks{datname!~'template.*'}[5m]) > 0
    for: 10m
    labels:
      severity: critical



### Kafka Event Hub
# - name: Kafka Event Hub
#   rules:
#   - alert: '[Kafka] Event hub No Leader'
#     annotations:
#       description: There is no ActiveController or 'leader' in the Kafka cluster.
#     expr: |
#       sum(kafka_controller_kafkacontroller_activecontrollercount) < 1
#     for: 5m
#     labels:
#       severity: critical
#   - alert: '[Kafka] Event hub Too Many Leaders'
#     annotations:
#       description: There is more than one ActiveController or 'leader' in the
#         Kafka cluster.
#     expr: |
#       kafka_controller_kafkacontroller_activecontrollercount > 1
#     for: 10m
#     labels:
#       severity: critical
#   - alert: '[Kafka] Event hub Offline Partitions'
#     annotations:
#       description: There are one or more Offline Partitions. These partitions
#         donâ€™t have an active leader and are hence not writable or readable.
#     expr: |
#       kafka_controller_kafkacontroller_offlinepartitionscount > 0
#     for: 5m
#     labels:
#       severity: critical
#   - alert: '[Kafka] Event hub Under Replicated Partitions'
#     annotations:
#       description: There are one or more Under Replicated Partitions.
#     expr: |
#       kafka_server_replicamanager_underreplicatedpartitions > 0
#     for: 10m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Under In-Sync Replicated Partitions'
#     annotations:
#       description: There are one or more Under In-Sync Replicated Partitions.
#         These partitions will be unavailable to producers who use 'acks=all'.
#     expr: |
#       kafka_cluster_partition_underreplicated > 0
#     for: 10m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub ConsumerGroup Lag Not Decreasing'
#     annotations:
#       description: The ConsumerGroup lag is not decreasing. The Consumers might
#         be down, failing to process the messages and continuously retrying, or
#         their consumption rate is lower than the production rate of messages.
#     expr: |
#       kafka_server_fetcherlagmetrics_consumerlag > 0 and delta(kafka_server_fetcherlagmetrics_consumerlag[2m]) >= 0
#     for: 15m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Consumergroup lag to large'
#     annotations:
#       description: Consumer group {{ $labels.consumergroup}} lag is too big ({{
#         $value }}) on topic {{ $labels.topic }}/partition {{ $labels.partition
#         }}
#       summary: Consumer group lag is too big
#     expr: kafka_server_fetcherlagmetrics_consumerlag > 1000
#     for: 10s
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Producer High ThrottleTime By Client-Id'
#     annotations:
#       description: The Producer has reached its quota and has high throttle time.
#         Applicable when Client-Id-only quotas are being used.
#     expr: |
#       max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Producer High ThrottleTime By User'
#     annotations:
#       description: The Producer has reached its quota and has high throttle time.
#         Applicable when User-only quotas are being used.
#     expr: |
#       max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Producer High ThrottleTime By User And Client-Id'
#     annotations:
#       description: The Producer has reached its quota and has high throttle time.
#         Applicable when Client-Id + User quotas are being used.
#     expr: |
#       max (kafka_server_producer__client_throttle_time) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Consumer High ThrottleTime By Client-Id'
#     annotations:
#       description: The Consumer has reached its quota and has high throttle time.
#         Applicable when Client-Id-only quotas are being used.
#     expr: |
#       max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Consumer High ThrottleTime By User'
#     annotations:
#       description: The Consumer has reached its quota and has high throttle time.
#         Applicable when User-only quotas are being used.
#     expr: |
#       max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Consumer High ThrottleTime By User And Client-Id'
#     annotations:
#       description: The Consumer has reached its quota and has high throttle time.
#         Applicable when Client-Id + User quotas are being used.
#     expr: |
#       max (kafka_consumer_fetch_manager_fetch_throttle_time_avg) > 1000
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Mirror Maker Container Down'
#     annotations:
#       description: All Kafka Mirror Maker containers have been down or in CrashLookBackOff
#         status for 3 minutes
#       summary: All Kafka Mirror Maker containers down or in CrashLookBackOff status
#     expr: absent(container_last_seen{container=~".+-mirrormaker2",pod=~".+-mirrormaker2.+"})
#     for: 3m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub Under Min Isr Partition Count'
#     annotations:
#       description: There are {{ $value }} partitions under the min ISR on {{ $labels.kubernetes_pod_name}}
#       summary: Kafka under min ISR partitions
#     expr: kafka_server_replicamanager_underminisrpartitioncount{cluster!="cpaas-dev-edg"} > 0
#     for: 10s
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub Cluster operator container down'
#     annotations:
#       description: The Cluster Operator has been down for longer than 90 seconds
#       summary: Cluster Operator down
#     expr: count((container_last_seen{container="strimzi-cluster-operator"} > (time()
#       - 90))) < 1 or absent(container_last_seen{container="strimzi-cluster-operator"})
#     for: 1m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub  KafkaContainerRestartedInTheLast5Minutes'
#     annotations:
#       description: One or more Kafka containers were restarted too often within
#         the last 5 minutes
#       summary: One or more Kafka containers restarted too often
#     expr: count(count_over_time(container_last_seen{container="kafka"}[5m])) >
#       2 * count(container_last_seen{container="kafka",pod=~".+-kafka-[0-9]+"})
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub AvgRequestLatency'
#     annotations:
#       description: The average request latency is {{ $value }} on {{ $labels.kubernetes_pod_name}}
#       summary: Zookeeper average request latency
#     expr: zookeeper_avgrequestlatency > 10
#     for: 10s
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub OutstandingRequests'
#     annotations:
#       description: There are {{ $value }} outstanding requests on {{ $labels.kubernetes_pod_name}}
#       summary: Zookeeper outstanding requests
#     expr: zookeeper_outstandingrequests > 10
#     for: 10s
#     labels:
#       severity: warning
#   - alert: '[Kafka Eventhub EDGE] Zookeeper Running Out Of Space less than 15%
#       free'
#     annotations:
#       description: There are only {{ $value }} bytes available at {{ $labels.persistentvolumeclaim}} PVC
#       summary: Zookeeper is running out of free disk space
#     expr: (kubelet_volume_stats_available_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"}
#       / kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~"data-(.+)-zookeeper-[0-9]+"})
#       < 0.15
#     for: 10s
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub ZookeeperContainerRestartedInTheLast5Minutes'
#     annotations:
#       description: One or more Zookeeper containers were restarted too often within
#         the last 5 minutes. This alert can be ignored when the Zookeeper cluster
#         is scaling up
#       summary: One or more Zookeeper containers were restarted too often
#     expr: count(count_over_time(container_last_seen{container="zookeeper"}[5m]))> 2 * count(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
#     for: 5m
#     labels:
#       severity: warning
#   - alert: '[Kafka] Event hub ZookeeperContainersDown'
#     annotations:
#       description: All zookeeper containers in the Zookeeper pods have been down
#         or in CrashLookBackOff status for 3 minutes
#       summary: All zookeeper containers in the Zookeeper pods down or in CrashLookBackOff
#         status
#     expr: absent(container_last_seen{container="zookeeper",pod=~".+-zookeeper-[0-9]+"})
#     for: 3m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub  TopicOperatorContainerDown'
#     annotations:
#       description: Container topic-operator in Entity Operator pod has been or
#         in CrashLookBackOff status for 3 minutes
#       summary: Container topic-operator in Entity Operator pod down or in CrashLookBackOff
#         status
#     expr: absent(container_last_seen{container="topic-operator",pod=~".+-entity-operator-.+"})
#     for: 3m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub UserOperatorContainerDown'
#     annotations:
#       description: Container user-operator in Entity Operator pod have been down
#         or in CrashLookBackOff status for 3 minutes
#       summary: Container user-operator in Entity Operator pod down or in CrashLookBackOff
#         status
#     expr: absent(container_last_seen{container="user-operator",pod=~".+-entity-operator-.+"})
#     for: 3m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub  EntityOperatorTlsSidecarContainerDown'
#     annotations:
#       description: Container tls-sidecar in Entity Operator pod have been down
#         or in CrashLookBackOff status for 3 minutes
#       summary: Container tls-sidecar Entity Operator pod down or in CrashLookBackOff
#         status
#     expr: absent(container_last_seen{container="tls-sidecar",pod=~".+-entity-operator-.+"})
#     for: 3m
#     labels:
#       severity: major
#   - alert: '[Kafka] Event hub ConnectContainersDown'
#     annotations:
#       description: All Kafka Connect containers have been down or in CrashLookBackOff
#         status for 3 minutes
#       summary: All Kafka Connect containers down or in CrashLookBackOff status
#     expr: absent(container_last_seen{container=~".+-connect",pod=~".+-connect-.+"})
#     for: 3m
#     labels:
#       severity: major


### Thanos side-cars
- name: thanos-sidecar
  rules:
  - alert: ThanosSidecarBucketOperationsFailed
    annotations:
      description: Thanos Sidecar {{$labels.instance}} bucket operations are failing
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanossidecarbucketoperationsfailed
      summary: Thanos Sidecar bucket operations are failing
    expr: |
      sum by (job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-sidecar.*"}[5m])) > 0
    for: 5m
    labels:
      severity: critical
  - alert: ThanosSidecarNoConnectionToStartedPrometheus
    annotations:
      description: Thanos Sidecar {{$labels.instance}} is unhealthy.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanossidecarnoconnectiontostartedprometheus
      summary: Thanos Sidecar cannot access Prometheus, even though Prometheus seems healthy and has reloaded WAL.
    expr: |
      thanos_sidecar_prometheus_up{job=~".*thanos-sidecar.*"} == 0
      AND on (namespace, pod)
      prometheus_tsdb_data_replay_duration_seconds != 0
    for: 5m
    labels:
      severity: critical

# Thanos Ruler
- name: thanos-rule
  rules:
  - alert: ThanosRuleQueueIsDroppingAlerts
    annotations:
      description: Thanos Rule {{$labels.instance}} is failing to queue alerts.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeueisdroppingalerts
      summary: Thanos Rule is failing to queue alerts.
    expr: |
      sum by (job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job=~".*thanos-rule.*"}[5m])) > 0
    for: 5m
    labels:
      severity: critical
  - alert: ThanosRuleSenderIsFailingAlerts
    annotations:
      description: Thanos Rule {{$labels.instance}} is failing to send alerts to alertmanager.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulesenderisfailingalerts
      summary: Thanos Rule is failing to send alerts to alertmanager.
    expr: |
      sum by (job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job=~".*thanos-rule.*"}[5m])) > 0
    for: 5m
    labels:
      severity: critical
  - alert: ThanosRuleHighRuleEvaluationFailures
    annotations:
      description: Thanos Rule {{$labels.instance}} is failing to evaluate rules.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationfailures
      summary: Thanos Rule is failing to evaluate rules.
    expr: |
      (
        sum by (job, instance) (rate(prometheus_rule_evaluation_failures_total{job=~".*thanos-rule.*"}[5m]))
      /
        sum by (job, instance) (rate(prometheus_rule_evaluations_total{job=~".*thanos-rule.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: critical
  - alert: ThanosRuleHighRuleEvaluationWarnings
    annotations:
      description: Thanos Rule {{$labels.instance}} has high number of evaluation warnings.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationwarnings
      summary: Thanos Rule has high number of evaluation warnings.
    expr: |
      sum by (job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job=~".*thanos-rule.*"}[10m])) > 1
    for: 15m
    labels:
      severity: info
  - alert: ThanosRuleRuleEvaluationLatencyHigh
    annotations:
      description: Thanos Rule {{$labels.instance}} has higher evaluation latency than interval for {{$labels.rule_group}}.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleruleevaluationlatencyhigh
      summary: Thanos Rule has high rule evaluation latency.
    expr: |
      (
        sum by (job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job=~".*thanos-rule.*"})
      >
        sum by (job, instance, rule_group) (prometheus_rule_group_interval_seconds{job=~".*thanos-rule.*"})
      )
    for: 5m
    labels:
      severity: warning
  - alert: ThanosRuleGrpcErrorRate
    annotations:
      description: Thanos Rule {{$labels.job}} is failing to handle {{$value | humanize}}% of requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulegrpcerrorrate
      summary: Thanos Rule is failing to handle grpc requests.
    expr: |
      (
        sum by (job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-rule.*"}[5m]))
      /
        sum by (job, instance) (rate(grpc_server_started_total{job=~".*thanos-rule.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: warning
  - alert: ThanosRuleConfigReloadFailure
    annotations:
      description: Thanos Rule {{$labels.job}} has not been able to reload its configuration.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleconfigreloadfailure
      summary: Thanos Rule has not been able to reload configuration.
    expr: avg by (job, instance) (thanos_rule_config_last_reload_successful{job=~".*thanos-rule.*"}) != 1
    for: 5m
    labels:
      severity: info
  - alert: ThanosRuleQueryHighDNSFailures
    annotations:
      description: Thanos Rule {{$labels.job}} has {{$value | humanize}}% of failing DNS queries for query endpoints.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeryhighdnsfailures
      summary: Thanos Rule is having high number of DNS failures.
    expr: |
      (
        sum by (job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job=~".*thanos-rule.*"}[5m]))
      /
        sum by (job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job=~".*thanos-rule.*"}[5m]))
      * 100 > 1
      )
    for: 15m
    labels:
      severity: warning
  - alert: ThanosRuleAlertmanagerHighDNSFailures
    annotations:
      description: Thanos Rule {{$labels.instance}} has {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulealertmanagerhighdnsfailures
      summary: Thanos Rule is having high number of DNS failures.
    expr: |
      (
        sum by (job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job=~".*thanos-rule.*"}[5m]))
      /
        sum by (job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job=~".*thanos-rule.*"}[5m]))
      * 100 > 1
      )
    for: 15m
    labels:
      severity: warning
  - alert: ThanosRuleNoEvaluationFor10Intervals
    annotations:
      description: Thanos Rule {{$labels.job}} has rule groups that did not evaluate for at least 10x of their expected interval.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulenoevaluationfor10intervals
      summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
    expr: |
      time() -  max by (job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job=~".*thanos-rule.*"})
      >
      10 * max by (job, instance, group) (prometheus_rule_group_interval_seconds{job=~".*thanos-rule.*"})
    for: 5m
    labels:
      severity: info
  - alert: ThanosNoRuleEvaluations
    annotations:
      description: Thanos Rule {{$labels.instance}} did not perform any rule evaluations in the past 10 minutes.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosnoruleevaluations
      summary: Thanos Rule did not perform any rule evaluations.
    expr: |
      sum by (job, instance) (rate(prometheus_rule_evaluations_total{job=~".*thanos-rule.*"}[5m])) <= 0
        and
      sum by (job, instance) (thanos_rule_loaded_rules{job=~".*thanos-rule.*"}) > 0
    for: 5m
    labels:
      severity: critical

#Thanos-store
- name: thanos-store
  rules:
  - alert: ThanosStoreGrpcErrorRate
    annotations:
      description: Thanos Store {{$labels.job}} is failing to handle {{$value | humanize}}% of requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosstoregrpcerrorrate
      summary: Thanos Store is failing to handle gRPC requests.
    expr: |
      (
        sum by (job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-store.*"}[5m]))
      /
        sum by (job) (rate(grpc_server_started_total{job=~".*thanos-store.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: warning
  - alert: ThanosStoreSeriesGateLatencyHigh
    annotations:
      description: Thanos Store {{$labels.job}} has a 99th percentile latency of {{$value}} seconds for store series gate requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosstoreseriesgatelatencyhigh
      summary: Thanos Store has high latency for store series gate requests.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(thanos_bucket_store_series_gate_duration_seconds_bucket{job=~".*thanos-store.*"}[5m]))) > 2
      and
        sum by (job) (rate(thanos_bucket_store_series_gate_duration_seconds_count{job=~".*thanos-store.*"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: warning
  - alert: ThanosStoreBucketHighOperationFailures
    annotations:
      description: Thanos Store {{$labels.job}} Bucket is failing to execute {{$value | humanize}}% of operations.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosstorebuckethighoperationfailures
      summary: Thanos Store Bucket is failing to execute operations.
    expr: |
      (
        sum by (job) (rate(thanos_objstore_bucket_operation_failures_total{job=~".*thanos-store.*"}[5m]))
      /
        sum by (job) (rate(thanos_objstore_bucket_operations_total{job=~".*thanos-store.*"}[5m]))
      * 100 > 5
      )
    for: 15m
    labels:
      severity: warning
  - alert: ThanosStoreObjstoreOperationLatencyHigh
    annotations:
      description: Thanos Store {{$labels.job}} Bucket has a 99th percentile latency of {{$value}} seconds for the bucket operations.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosstoreobjstoreoperationlatencyhigh
      summary: Thanos Store is having high latency for bucket operations.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(thanos_objstore_bucket_operation_duration_seconds_bucket{job=~".*thanos-store.*"}[5m]))) > 2
      and
        sum by (job) (rate(thanos_objstore_bucket_operation_duration_seconds_count{job=~".*thanos-store.*"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: warning

# Thanos Query
- name: thanos-query
  rules:
  - alert: ThanosQueryHttpRequestQueryErrorRateHigh
    annotations:
      description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of "query" requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryerrorratehigh
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query"}[5m]))
      /
        sum by (job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: critical
  - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
    annotations:
      description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of "query_range" requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryrangeerrorratehigh
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(http_requests_total{code=~"5..", job=~".*thanos-query.*", handler="query_range"}[5m]))
      /
        sum by (job) (rate(http_requests_total{job=~".*thanos-query.*", handler="query_range"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: critical
  - alert: ThanosQueryGrpcServerErrorRate
    annotations:
      description: Thanos Query {{$labels.job}} is failing to handle {{$value | humanize}}% of requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcservererrorrate
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*thanos-query.*"}[5m]))
      /
        sum by (job) (rate(grpc_server_started_total{job=~".*thanos-query.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: warning
  - alert: ThanosQueryGrpcClientErrorRate
    annotations:
      description: Thanos Query {{$labels.job}} is failing to send {{$value | humanize}}% of requests.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcclienterrorrate
      summary: Thanos Query is failing to send requests.
    expr: |
      (
        sum by (job) (rate(grpc_client_handled_total{grpc_code!="OK", job=~".*thanos-query.*"}[5m]))
      /
        sum by (job) (rate(grpc_client_started_total{job=~".*thanos-query.*"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: warning
  - alert: ThanosQueryHighDNSFailures
    annotations:
      description: Thanos Query {{$labels.job}} have {{$value | humanize}}% of failing DNS queries for store endpoints.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhighdnsfailures
      summary: Thanos Query is having high number of DNS failures.
    expr: |
      (
        sum by (job) (rate(thanos_query_store_apis_dns_failures_total{job=~".*thanos-query.*"}[5m]))
      /
        sum by (job) (rate(thanos_query_store_apis_dns_lookups_total{job=~".*thanos-query.*"}[5m]))
      ) * 100 > 1
    for: 15m
    labels:
      severity: warning
  - alert: ThanosQueryInstantLatencyHigh
    annotations:
      description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}} seconds for instant queries.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryinstantlatencyhigh
      summary: Thanos Query has high latency for queries.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m]))) > 40
      and
        sum by (job) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: critical
  - alert: ThanosQueryRangeLatencyHigh
    annotations:
      description: Thanos Query {{$labels.job}} has a 99th percentile latency of {{$value}} seconds for range queries.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryrangelatencyhigh
      summary: Thanos Query has high latency for queries.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*thanos-query.*", handler="query_range"}[5m]))) > 90
      and
        sum by (job) (rate(http_request_duration_seconds_count{job=~".*thanos-query.*", handler="query_range"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: critical
  - alert: ThanosQueryOverload
    annotations:
      description: Thanos Query {{$labels.job}} has been overloaded for more than 15 minutes. This may be a symptom of excessive simultanous complex requests, low performance of the Prometheus API, or failures within these components. Assess the health of the Thanos query instances, the connnected Prometheus instances, look for potential senders of these requests and then contact support.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryoverload
      summary: Thanos query reaches its maximum capacity serving concurrent requests.
    expr: |
      (
        max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
      )
    for: 15m
    labels:
      severity: warning

# Thanos absent components
- name: thanos-component-absent
  rules:
  - alert: ThanosCompactIsDown
    annotations:
      description: ThanosCompact has disappeared. Prometheus target for the component cannot be discovered.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanoscompactisdown
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*thanos-compact.*"} == 1)
    for: 5m
    labels:
      severity: critical
  - alert: ThanosQueryIsDown
    annotations:
      description: ThanosQuery has disappeared. Prometheus target for the component cannot be discovered.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryisdown
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*thanos-query.*"} == 1)
    for: 5m
    labels:
      severity: critical
  - alert: ThanosRuleIsDown
    annotations:
      description: ThanosRule has disappeared. Prometheus target for the component cannot be discovered.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleisdown
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*thanos-rule.*"} == 1)
    for: 5m
    labels:
      severity: critical
  - alert: ThanosStoreIsDown
    annotations:
      description: ThanosStore has disappeared. Prometheus target for the component cannot be discovered.
      runbook_url: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosstoreisdown
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*thanos-store.*"} == 1)
    for: 5m
    labels:
      severity: critical